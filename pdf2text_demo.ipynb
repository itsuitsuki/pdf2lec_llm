{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import encode_image_to_base64\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import fitz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_image_gpt4o(prompt, image_path):\n",
    "    \"\"\"\n",
    "    Analyze an image using the GPT-4o model and return a description.\n",
    "\n",
    "    :param prompt: The text prompt for the model\n",
    "    :param image_path: Local path to the image file\n",
    "    :return: Model-generated description or error message\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Encode the image\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                                \"detail\": \"high\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    Convert each page of a PDF file to an image and save them to the specified output directory.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file\n",
    "    :param output_dir: Directory to save the converted images\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    pdf_name = Path(pdf_path).stem\n",
    "    image_dir = Path(output_dir) / pdf_name\n",
    "    image_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Open the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_num, page in enumerate(doc):\n",
    "        # Convert the page to an image\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))  # 300 DPI\n",
    "        \n",
    "        # Save the image\n",
    "        image_filename = f\"page_{page_num+1}.png\"\n",
    "        pix.save(image_dir / image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = \"L15-nearest-neighbor-10-17\"\n",
    "pdf_path = f\"./data/test_pdfs/{pdf_name}.pdf\"\n",
    "output_dir = f\"./data/test_images/\"\n",
    "convert_pdf_to_images(pdf_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged images saved to ./data/test_images/L15-nearest-neighbor-10-17_merged\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def calculate_similarity(img1, img2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two images using ORB feature matching.\n",
    "    This method is invariant to translation and rotation.\n",
    "    \n",
    "    :param img1: First image\n",
    "    :param img2: Second image\n",
    "    :return: A similarity score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "    \n",
    "    # Find the keypoints and descriptors with ORB\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "    \n",
    "    # Create BFMatcher object\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    # Match descriptors\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    # Sort them in the order of their distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    similarity = len(matches) / max(len(kp1), len(kp2))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def merge_similar_images(image_dir, output_dir, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Merge similar consecutive images in a directory while maintaining the original order.\n",
    "    \n",
    "    :param image_dir: Directory containing the images\n",
    "    :param output_dir: Directory to save the merged images\n",
    "    :param similarity_threshold: Threshold for considering images as similar\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files sorted by name\n",
    "    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "    \n",
    "    merged_groups = []\n",
    "    current_group = [image_files[0]]\n",
    "    \n",
    "    for i in range(len(image_files) - 1):\n",
    "        img1 = cv2.imread(os.path.join(image_dir, image_files[i]))\n",
    "        img2 = cv2.imread(os.path.join(image_dir, image_files[i+1]))\n",
    "        \n",
    "        similarity = calculate_similarity(img1, img2)\n",
    "        \n",
    "        if similarity >= similarity_threshold:\n",
    "            current_group.append(image_files[i+1])\n",
    "        else:\n",
    "            merged_groups.append(current_group)\n",
    "            current_group = [image_files[i+1]]\n",
    "    \n",
    "    # Add the last group\n",
    "    if current_group:\n",
    "        merged_groups.append(current_group)\n",
    "    \n",
    "    # Merge and save images\n",
    "    for i, group in enumerate(merged_groups):\n",
    "        if len(group) == 1:\n",
    "            img = cv2.imread(os.path.join(image_dir, group[0]))\n",
    "            merged = img\n",
    "        else:\n",
    "            images = [cv2.imread(os.path.join(image_dir, f)) for f in group]\n",
    "            heights = [img.shape[0] for img in images]\n",
    "            max_width = max(img.shape[1] for img in images)\n",
    "            merged = np.vstack([cv2.resize(img, (max_width, img.shape[0])) for img in images])\n",
    "        \n",
    "        # Use the first image's number in the group for naming\n",
    "        first_num = int(group[0].split('_')[1].split('.')[0])\n",
    "        cv2.imwrite(os.path.join(output_dir, f'merged_{first_num:03d}.png'), merged)\n",
    "    \n",
    "    print(f\"Merged images saved to {output_dir}\")\n",
    "\n",
    "image_dir = f\"./data/test_images/{pdf_name}\"\n",
    "output_dir = f\"./data/test_images/{pdf_name}_merged\"\n",
    "merge_similar_images(image_dir, output_dir, similarity_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_prompt_path = './prompts/slide_prompt'\n",
    "with open(lecture_prompt_path, 'r') as file:\n",
    "    lecture_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lecture_from_images(image_dir, prompt):\n",
    "    \"\"\"\n",
    "    Generate a complete lecture by analyzing images in sequence, maintaining context.\n",
    "    \n",
    "    :param image_dir: Directory containing the merged images\n",
    "    :param prompt: The base prompt to use for image analysis\n",
    "    :return: Complete lecture content\n",
    "    \"\"\"\n",
    "    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "    full_lecture = \"\"\n",
    "    context = []\n",
    "    \n",
    "    for i, image_file in tqdm(enumerate(image_files)):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        \n",
    "        # Create a context-aware prompt\n",
    "        context_prompt = f\"{prompt}\\n\\nContext from previous slides:\\n{' '.join(context)}\\n\\nAnalyze the current slide in the context of what has been discussed before. remember do not repeat the same information.\"\n",
    "        \n",
    "        slide_content = analyze_image_gpt4o(context_prompt, image_path)\n",
    "        full_lecture += f\"\\n\\n--- Slide: {image_file} ---\\n{slide_content}\"\n",
    "        \n",
    "        # Update context\n",
    "        context.append(slide_content)\n",
    "        if len(context) > 2:\n",
    "            context.pop(0)\n",
    "    \n",
    "    return full_lecture\n",
    "merged_image_dir = f\"./data/test_images/{pdf_name}_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L15-nearest-neighbor-10-17'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:24,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Slide: merged_001.png ---\n",
      "Let's dive into today's fascinating topic: the difference between generative and discriminative models in the realm of classification. This is a crucial area of study in machine learning and statistics, providing us with distinct strategies to solve classification problems.\n",
      "\n",
      "First, let's define these terms clearly. A classification task in machine learning is about predicting a category or class label for a given input data. Now, how do generative and discriminative models approach this?\n",
      "\n",
      "Generative models are like learning the entire story. They try to model the joint probability distribution of the data features and the labels. Essentially, they try to understand how the data is generated, thus allowing them to both classify and generate new samples. Common examples include Gaussian Mixture Models and Hidden Markov Models. Imagine having a magic recipe book that not only tells you what dish you're making based on ingredients but also allows you to tweak or create new dishes entirely.\n",
      "\n",
      "On the other hand, discriminative models focus solely on the boundary between different classes. They don't concern themselves with how the data was generated. Instead, they try to learn the direct mapping from input features to the class labels, effectively drawing a decision boundary. Think of discriminative models as a diligent referee who only marks whether a player is in or out of bounds. Popular examples include Logistic Regression and Support Vector Machines.\n",
      "\n",
      "Now, why would you choose one over the other? If you need a model that can give you insights into the underlying process or generate new data points, a generative model is your best bet\n",
      "\n",
      "--- Slide: merged_002.png ---\n",
      "As we delve deeper into classification, let's focus on a few key differences and specifics that set this apart from other types of machine learning problems, like regression.\n",
      "\n",
      "First, unlike regression, where our goal is to predict a continuous value, classification is all about predicting discrete labels or categories. Think of it like sorting items into predefined boxes. For instance, you might classify emails as \"spam\" or \"not spam.\"\n",
      "\n",
      "Now, let's talk about the nature of these labels. They can be binary, which means there are only two possible categories. A practical example here would be medical diagnostics, such as determining if cholesterol levels indicate a \"healthy\" or \"unhealthy\" status. Similarly, in bioinformatics, you might predict whether two proteins interact or not — again, a yes or no situation.\n",
      "\n",
      "However, classification can also involve more than two categories. This is where we encounter multi-class classification problems. Take, for example, the task of recognizing handwritten digits. Here, each image represents a number from zero to nine. So, instead of two potential labels, we have ten!\n",
      "\n",
      "A critical aspect of classification is the data set we work with. We're given pairs of inputs and their corresponding class labels, and our job is to learn a mapping function. This function essentially tunes itself to accurately predict the class label for any given input. It's like training a dog to fetch the right toy on command. Each toy is a class, and fetching success is about getting the right one every time.\n",
      "\n",
      "So, in essence\n",
      "\n",
      "--- Slide: merged_003.png ---\n",
      "Now, let's focus on an essential component of generative models: class-conditional probabilities. On the slide, you see two probability distributions, one in blue and one in red. These represent class-conditional probabilities, a crucial concept in our understanding of how generative models operate.\n",
      "\n",
      "Class-conditional probability is the probability of a data feature given a particular class label. Put simply, it's like asking, \"Given that we know something belongs to a certain category, what are the chances that it has these particular features?\" \n",
      "\n",
      "For instance, imagine you’re deciding whether a fruit is an apple or an orange based on color. The class-conditional probability would be about understanding the likelihood of a fruit being red if it's an apple as opposed to an orange. \n",
      "\n",
      "In the graph, notice those peaks? They show where our features — in this case, labeled as \"x\" — are most likely to exist for each class. The blue curve could represent apples, while the red curve might signify oranges. So, if a feature value lands where the red curve peaks, it's most likely an orange.\n",
      "\n",
      "Why are these probabilities important? In generative models, knowing these probabilities helps us compute how likely data is generated under different assumptions — or classes. Think of it as having two different glasses to look through. With the red and blue glass, you can better predict and grasp the nuances of each class.\n",
      "\n",
      "And how do we know these probabilities? They can be estimated from existing data, a fundamental part of training gener\n",
      "\n",
      "--- Slide: merged_004.png ---\n",
      "Now, on this slide, we're diving into Bayes' Rule. This is a cornerstone of probability theory, and it’s particularly valuable when we're dealing with classification problems in machine learning.\n",
      "\n",
      "First, let's define Bayes' Rule in a simplified manner. Essentially, it helps us update our beliefs about the likelihood of a class given some observed data. It's a bit like updating your opinion about a suspect's guilt when new evidence is presented.\n",
      "\n",
      "In formal terms, Bayes' Rule works with probabilities. We have class-conditional probabilities, which we discussed earlier. These probabilities tell us how likely a certain observation is, given a known class. Now, what Bayes' Rule does is flip this around to find what's called the posterior probability — the likelihood of a class given the observation.\n",
      "\n",
      "Let’s break it down using the notation on the slide. If we have a feature, say \"x,\" the posterior probability of class '0' given 'x' is calculated as follows: \n",
      "\n",
      "- We take the probability of 'x' given class '0', which is a class-conditional probability, and multiply it by the prior probability of class '0'. The prior probability is our initial assumption about the class occurrence before seeing the data.\n",
      "- We then divide by the probability of observing 'x', regardless of class, which ensures our results are normalized.\n",
      "\n",
      "Similarly, for class '1', we apply the same logic. You can think of this as updating our odds: initially guessing based on what we know,\n",
      "\n",
      "--- Slide: merged_005.png ---\n",
      "Building on our understanding of class-conditional probabilities, let’s delve into an equally fascinating concept: posterior probabilities. \n",
      "\n",
      "On this slide, you're presented with two curves representing posterior probabilities, labeled as p of zero given x, and p of one given x — in blue and red, respectively. Now, what do these curves tell us?\n",
      "\n",
      "Posterior probability is essentially the updated belief of a class being true after considering new evidence. You see, while class-conditional probability tells us the likelihood of observing our data given a class, posterior probability flips it — it tells us the likelihood of a class given our data.\n",
      "\n",
      "Think of it this way: Imagine you're a detective again. You have two suspects based on evidence collected at the scene — suspect zero and suspect one. The blue and red curves essentially show how our belief in each suspect's guilt changes as more evidence (or our feature value x) comes into play.\n",
      "\n",
      "Notice how the curves transition? This is where Bayes' Rule shines. As new data rolls in, our certainty about which class our data point belongs to becomes clearer. Initially, say on the far left, the blue curve is one, meaning class zero is almost certain. But as we move right, crossing that pivotal midpoint, our confidence shifts dramatically towards class one, represented by the red curve.\n",
      "\n",
      "So, why is this crucial? In machine learning, particularly in classification tasks, this back-and-forth calculation of probabilities helps models make better, more informed predictions. It's a dance of precision\n",
      "\n",
      "--- Slide: merged_006.png ---\n",
      "Let's dive into the concept of the decision boundary in classification. The idea here is that we need a rule to decide between different classes based on the probabilities we've been discussing.\n",
      "\n",
      "Now, the decision boundary is essentially the line or region where we're unsure which class a data point belongs to, and it depends heavily on the loss function we've chosen. The loss function is a mathematical way to measure how costly it is to make mistakes. In this case, it helps us determine how we manage the trade-off between different kinds of errors, like false negatives and false positives.\n",
      "\n",
      "So, how do we make this decision optimally? The rule of thumb is to choose the class where the probability of that class given the data is maximal. In simpler terms, it's about choosing the option with the highest posterior probability. \n",
      "\n",
      "The notation here, represented as k equals the value that maximizes the probability of k given x, illustrates that we're hunting for the highest probability class. Why? Because this minimizes our chance of making a mistake, or what we call the probability of error.\n",
      "\n",
      "To find this error, we integrate the probability of making such an error across all possible data points. You're balancing the risks, ensuring that your decision isn't just a shot in the dark but rather an informed choice based on probability distribution.\n",
      "\n",
      "However, here's a crucial point: minimizing error by this method isn't always suitable, particularly in safety-critical scenarios. Imagine a medical diagnosis system—here, false negatives, where you miss a critical condition, could be far more\n",
      "\n",
      "--- Slide: merged_007.png ---\n",
      "Now that we've familiarized ourselves with class-conditional and posterior probabilities, let's explore the three primary ways to build classifiers, as highlighted on this slide. \n",
      "\n",
      "First, we have the **generative approach**. This involves modeling the class-conditional probabilities. Essentially, this is the probability of observing certain data, given a specific class. After that, we model the prior probability, which is our initial belief about the class distribution before seeing any data. Think of this as the initial assumption about how likely each class is, even without any evidence.\n",
      "\n",
      "Once we have these probabilities, we use **Bayes' Rule** to obtain the posterior probabilities. Imagine we're detectives again — we start with a general sense or a guess about who might be the suspect based on prior experience. As evidence piles up, we refine our suspicions using Bayes' Rule to update our guesses.\n",
      "\n",
      "Next is the **discriminative approach**. Here, instead of modeling the data for each class, we directly model the posterior probabilities. This means we're jumping straight to determining the probability of a class given the data without delving into the specifics of how each piece of evidence comes into play.\n",
      "\n",
      "Finally, we have the crucial step of finding **decision boundaries**. This is about demarcating where one class ends and another begins in our data space. Decision boundaries are like invisible fences that help our model decide which side, or class, a data point belongs to. We utilize something known as **Decision Theory** to perform this\n",
      "\n",
      "--- Slide: merged_008.png ---\n",
      "Let's explore the fascinating world of logistic regression. This is an essential tool in classification tasks, and what makes it intriguing is that it offers a probabilistic approach to predicting which class a data point belongs to. But remember—logistic regression is unique in that it provides a binary output. In other words, the outcome is either one class or the other, not a continuous value.\n",
      "\n",
      "Now, how does logistic regression work? At its core, it utilizes what we call a **logistic function** to model the posterior probability. The beauty of this function is that it maps any real-valued number into a value between zero and one, perfect for probability estimates.\n",
      "\n",
      "So, imagine plotting data on a graph. We use a linear function of our features as input into the logistic function. This results in an S-shaped curve, which smoothly transitions between classes. This smooth transition is valuable because it allows for a more nuanced classification around the decision boundary we previously discussed.\n",
      "\n",
      "You might be wondering why this specific approach? Logistic regression can be justified in numerous ways, and soon we’ll delve into how it’s derived for Gaussian class-conditional densities—a topic that adds depth to our understanding.\n",
      "\n",
      "Also, although logistic regression typically deals with two classes, it can indeed handle more. We simply extend it to what's known as **multinomial logistic regression** when dealing with more than two classes. This versatility makes it a widely-used method in many practical applications.\n",
      "\n",
      "In conclusion, logistic regression offers a robust framework for making informed,\n",
      "\n",
      "--- Slide: merged_009.png ---\n",
      "Alright, let's dive into the current slide and unravel what's going on.\n",
      "\n",
      "Here, we’re examining the **posterior probability for Gaussian class-conditional densities**. This might sound a bit complex, but we'll break it down step by step.\n",
      "\n",
      "First, an **important assumption** we’re making is that the variances for different classes are the same. This simplifies our calculations significantly.\n",
      "\n",
      "Now, we begin with one-dimensional feature vectors. Essentially, we're analyzing data that can be plotted on a single line, or axis, which is less complicated than higher dimensions. However, the principles we discuss here will also apply to multi-dimensional cases — it's just that the math gets a tad more involved.\n",
      "\n",
      "In this specific context, our goal is to discover that the posterior probability can be expressed as a **logistic function**, which is also known as the sigmoid function. It's a special type of mathematical function that converts any input number into a value between zero and one. Perfect for probabilities, right?\n",
      "\n",
      "Let’s take a closer look at the equation mentioned: the variable **z** is defined as a linear combination of inputs, particularly a weighted sum of a feature plus a bias term. Picture this as a line on a graph that we discussed in logistic regression, creating that familiar S-shaped curve.\n",
      "\n",
      "Now, why does the logistic function work here? Well, the slide points out that the logistic function is **strictly monotonically increasing**. That's a fancy way of saying that as the input increases, the output always increases\n",
      "\n",
      "--- Slide: merged_010.png ---\n",
      "Let's delve into the current slide, where we're focusing on refining our understanding of logistic regression through the lens of Gaussian class-conditional densities.\n",
      "\n",
      "One critical assumption in our analysis here is that the variances, denoted by the Greek letter sigma squared, are consistent across both classes. This equal variance assumption simplifies our mathematical computations significantly.\n",
      "\n",
      "Now, on this slide, we see the formula for the posterior probability: the probability of one class given the input, divided by the sum of the probabilities of both classes, each weighted by their prior probabilities. Essentially, this gives us the likelihood that a given data point belongs to a specific class.\n",
      "\n",
      "To compute the probability of class one given our feature, we subtract the probability of class zero from one. We use the logistic function to elegantly reframe this probability: one divided by one plus the exponential function raised to the negative z power. Here, z is a linear combination of our features — essentially, the weighted sum of our input feature plus a bias or intercept term.\n",
      "\n",
      "The beauty of the logistic function in this context is its ability to translate linear boundaries into a probabilistic framework. Our z equation, which combines theta values — think of these as parameters or coefficients — along with our input feature, captures this transformation.\n",
      "\n",
      "By fitting our model, we fine-tune these theta parameters. It's interesting to note here that if you want to determine the parameters related to one class conditioned on another, you simply flip the signs of the thetas associated with the opposite class. This elegant\n",
      "\n",
      "--- Slide: merged_011.png ---\n",
      "Now, let’s delve into this slide where we explore the graph of the logistic function, focusing on how this relates to our understanding of Gaussian class-conditional densities and posterior probabilities.\n",
      "\n",
      "Notice the graph here showcases the logistic function as a function of \\( z \\). As we’ve discussed, \\( z \\) represents our linear combination of inputs — think of it as that straight line we get from our feature’s weighted sum plus the all-important bias term. This is what we refer to as an \"affine function.\" In simpler terms, it’s like a linear function but with a shift.\n",
      "\n",
      "When we generalize this idea to multivariate Gaussians, our feature vectors become multi-dimensional, represented as \\( x \\) in uppercase. This means instead of working with single-line data, we're playing on a multi-dimensional field. Our affine transformation now involves multiplying the entire feature vector \\( x \\) by a vector of weights, denoted here as theta, and adding our bias.\n",
      "\n",
      "The task here is proving that the posterior turns out to be a logistic function of this affine transformation. And why is this important? Because it translates our linear inputs into probabilities that smoothly transition between zero and one, thanks to the logistic function’s characteristic S-shape.\n",
      "\n",
      "This graphical interpretation offers a visual understanding of how our parameters influence the resulting probability through the logistic transformation. As \\( z \\) increases, the curve rises sharply, moving from probabilities close to zero to those nearing one — a pivotal feature that encapsulates the balance\n",
      "\n",
      "--- Slide: merged_012.png ---\n",
      "Now, let's take a closer look at this slide, which continues to build on our understanding of logistic regression through class-conditional densities.\n",
      "\n",
      "What we're seeing here is a derivation — a step-by-step mathematical journey — leading us to the logistic function, a cornerstone of logistic regression. \n",
      "\n",
      "We start by looking at the log of the ratio of the probabilities of class zero to class one, given a feature vector. This is known as the log odds or sometimes referred to as the logit. The equation starts with a complex-looking expression involving means, variances, and covariances, capturing the differences between the average points — mu zero and mu one — for our two classes, weighted by the inverse of the covariance matrix. \n",
      "\n",
      "In simpler terms, this expression quantifies how distinguishable our two classes are, based on the distances and the spread of their distributions. The colorful part of the equation highlights the transformation of these differences into the theta parameters, denoting the slope of our linear boundary in this probabilistic space.\n",
      "\n",
      "Interestingly, the rightmost part includes prior probabilities — a nod to Bayesian thinking, illuminating how these initial beliefs shape our final estimates.\n",
      "\n",
      "Now, notice the simplified expression: it's rewritten as theta transposed, multiplied by x-tilde — a reformulation that simplifies our math and reveals an elegant symmetry. Here, x-tilde includes a one to represent the intercept term alongside our feature x, while theta-tilde encapsulates all our parameters together. \n",
      "\n",
      "Moving forward, the\n",
      "\n",
      "--- Slide: merged_013.png ---\n",
      "Let's dive into the core concepts presented in this slide. Here, we're investigating a key aspect of logistic regression: how we can transition from a simple affine function to modeling posterior probabilities through a logistic function. \n",
      "\n",
      "The slide introduces us to a fundamental question: why can't we directly use affine functions to model posterior probabilities? The crux of the issue lies in the range of linear functions. These functions can theoretically extend from negative infinity to positive infinity. However, probabilities must fit neatly between zero and one. This clash necessitates a transformation. \n",
      "\n",
      "So, what can we do? This is where the odds come into play. By focusing on the odds ratio — defined as the probability of an event occurring divided by the probability of it not occurring — we find a much friendlier range. These odds can span from zero to positive infinity. To pull this into a more manageable and mathematically elegant form, we take the logarithm of the odds. This transformation allows the log of the odds, also known as the logit, to span from negative infinity to positive infinity.\n",
      "\n",
      "By doing this, we set the stage for linear modeling. The logit, that logarithm of the odds, is equated to a linear function of our inputs. In this way, we've elegantly transformed the problem so that we can leverage our linear tools to predict probabilities.\n",
      "\n",
      "The takeaway here is subtle yet powerful — by using this transformation, we're able to apply linear models to a task that initially didn’t seem suitable. This conversion is what\n",
      "\n",
      "--- Slide: merged_014.png ---\n",
      "Now, let's delve into this slide, which shines a spotlight on the remarkable utility of the logistic function within the field of neural networks. During the years 1989 to 1993, an exciting discovery was made regarding the universality of function approximation using logistic sigmoid functions in neural networks.\n",
      "\n",
      "To break this down, let's start by defining the logistic sigmoid function. It's a type of mathematical function that maps any real-valued number into a range between zero and one, smoothly transforming inputs into outputs in a way that's particularly useful for modeling probabilities. \n",
      "\n",
      "In the context of neural networks, a logistic sigmoid activation function helps determine how strongly an input influences the result. Imagine you have a neuron that takes in some inputs — let's say, x1, x2, and x3 — and it computes a weighted sum. The logistic sigmoid function then transforms this sum into something more manageable: a value between zero and one. This simplicity and robustness make it a favorite in early neural networks and even some modern applications.\n",
      "\n",
      "This historical discovery underscored that by stacking these logistic sigmoid functions together, one could approximate a wide variety of functions — hence the term \"universality.\" Essentially, by layering multiple sigmoid functions, a network can model complex, non-linear relationships. That architectural concept laid the groundwork for modern neural network designs, leading to breakthroughs in understanding and predicting intricate patterns in data, from image recognition to natural language processing.\n",
      "\n",
      "The diagram on the slide illustrates this beautifully, showing how inputs interact through different layers, each applying\n",
      "\n",
      "--- Slide: merged_015.png ---\n",
      "Let's explore the concepts presented in this slide further. We're diving into the heart of how we model the posterior probability distribution in logistic regression, specifically focusing on binary classification, where our outcomes are either zero or one.\n",
      "\n",
      "First, we see here that we treat our class label, called Y, as a Bernoulli random variable. Now, what does that mean? Well, a Bernoulli random variable represents a random experiment with only two possible outcomes. Think of it like a coin toss where you can only get heads or tails.\n",
      "\n",
      "In logistic regression, we are interested in modeling the probability that our class label Y equals one, given some input features, which we denote as x. This probability is expressed using the logistic function. On this slide, it's given by the equation: the probability that Y equals one given x is one divided by one plus the exponential of the negative sum of a weighted input, subtracted from a constant term. This formula is central because it maps the input space into a probability range between zero and one. The result, μ of x, is our modeled probability.\n",
      "\n",
      "Now, as we delve deeper, notice the formula for a binary case where y represents the outcomes of our Bernoulli variable, either zero or one. Here, we express the probability of observing y given x as μ of x to the power of y, times one minus μ of x to the power of one minus y. This formula crafts a likelihood based on whether our observed outcome is zero or one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "complete_lecture = generate_lecture_from_images(merged_image_dir, lecture_prompt)\n",
    "print(complete_lecture)\n",
    "# Optionally, save the lecture to a file\n",
    "with open(f\"./data/generated_lectures/{pdf_name}_lecture.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(complete_lecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_slide_prompt_path = './prompts/final_slide_prompt'\n",
    "with open(final_slide_prompt_path, 'r') as file:\n",
    "    final_slide_prompt = file.read()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture content and summary have been saved to 'lecture_and_summary.txt'\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def summarize_lecture(complete_lecture, final_slide_prompt):\n",
    "    client = OpenAI()\n",
    "\n",
    "    summary = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": complete_lecture + '\\n\\n' + final_slide_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return summary.choices[0].message.content\n",
    "\n",
    "\n",
    "summary = summarize_lecture(complete_lecture, final_slide_prompt)\n",
    "\n",
    "# 将完整讲座内容和摘要写入文本文件\n",
    "with open('lecture_and_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Complete Lecture:\\n\\n\")\n",
    "    f.write(complete_lecture)\n",
    "    f.write(\"\\n\\nSummary:\\n\\n\")\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"Lecture content and summary have been saved to 'lecture_and_summary.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
